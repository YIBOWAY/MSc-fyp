{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eba3e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e3948",
   "metadata": {},
   "source": [
    "### ADD Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea44ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 3\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            # Add dropout layer\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ea588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70ce934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # adjust num_labels\n",
    "model = BertClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9faabdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_data(folder):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(folder, filename)) as f:\n",
    "                file_data = json.load(f)\n",
    "                for article in file_data['articles']:\n",
    "                    data.append((article['title'] + ' ' + article['content'], file_data['label_text']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d178ca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Joe Biden’s Lying Anti-Trump Ad is STILL on Twitter: Gets Highest Fake News Rating of “Four Pinocchios” Presidential candidate Joe Biden has a well-known history of lying and plagiarism .\\nIt looks like his campaign is getting in on the act with their latest ad that just got the worst fake news rating possible .\\nThe Washington Post gave Biden ’ s new ad “ Four Pinocchios ” for “ manipulating video ” to make it appear as though President Trump called the coronavirus a hoax .\\nThe Biden campaign cut out over 120 words in between the word “ coronavirus ” and then “ This is their new hoax.\\n” ( see transcript below ) In  saying “ coronavirus , ” followed immediately by “ This is their new hoax.\\n” What the president was saying is that the Democratic politicization of the coronavirus is a “ hoax ” and NOT the virus itself .\\nThe ad goes on to show images and words that are disconnected and made to make it seem like the president said “ The American Dream ” … ” is dead.\\n” This is gutter politics and is still on Twitter !\\nThe full transcript including what the Biden campaign left out is below  the coronavirus .\\nYou know that , right ?\\nCoronavirus .\\nThey ’ re politicizing it .\\nWe did one of the great jobs , you say , ‘ How ’ s President Trump doing ?\\n’ , ‘ Oh , nothing , nothing.\\n’ They have no clue , they don ’ t have any clue .\\nThey can ’ t even count their votes in Iowa , they can ’ t even count .\\nNo , they can ’ t .\\nThey can ’ t count their votes .\\nOne of  said , ‘ Mr .\\nPresident , they tried to beat you on Russia , Russia , Russia.\\n’ That didn ’ t work out too well .\\nThey couldn ’ t do it .\\nThey tried the impeachment hoax .\\nThat was on a perfect conversation .\\nThey tried anything , they tried it over and over , they ’ ve been doing it since he got in .\\nIt ’ s all turning , they lost .\\nIt ’ s all turning , think of it , think of it .\\n But you know we did something that ’ s been pretty amazing .\\nWe have 15 people in this massive country and because of the fact that we went early , we went early , we could have had a lot more than that.\\n” Biden ’ s problem with the truth has been completely ignored by the media .\\nThey blow it off like it doesn ’ t matter .\\nIt ’ s just old “ Uncle Joe ” telling stories again .\\nWhen Biden ’ s campaign is lying about President  race , IT MATTERS .', 0)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data('../final_project/datasets/dataset_fake_news_task4/train_json')  \n",
    "test_data = load_data('../final_project/datasets/dataset_fake_news_task4/dev_json')  \n",
    "\n",
    "# Convert labels to integers\n",
    "label_mapping = {'low': 0, 'mixed': 1, 'high': 2}\n",
    "train_data = [(text, label_mapping[label]) for text, label in train_data]\n",
    "test_data = [(text, label_mapping[label]) for text, label in test_data]\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11558f6f",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4cf2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "train_dataset = pd.read_csv('../final_project/datasets/train_dataset.csv')\n",
    "dev_dataset = pd.read_csv('../final_project/datasets/dev_dataset.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "train_data = [(text, label) for text, label in zip(train_dataset['text'], train_dataset['label'])]\n",
    "test_data = [(text, label) for text, label in zip(dev_dataset['text'], dev_dataset['label'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6f6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data and convert to tensors\n",
    "train_encodings = tokenizer([text for text, label in train_data], truncation=True, padding=True, max_length=512)\n",
    "train_labels = torch.tensor([label for text, label in train_data])\n",
    "test_encodings = tokenizer([text for text, label in test_data], truncation=True, padding=True, max_length=512)\n",
    "test_labels = torch.tensor([label for text, label in test_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f075fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_encodings['input_ids'] = torch.tensor(train_encodings['input_ids'])\n",
    "train_encodings['attention_mask'] = torch.tensor(train_encodings['attention_mask'])\n",
    "test_encodings['input_ids'] = torch.tensor(test_encodings['input_ids'])\n",
    "test_encodings['attention_mask'] = torch.tensor(test_encodings['attention_mask'])\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7f94d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0009, 0.0005, 0.0002], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "class_weights = class_weights.to(device)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5113fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2edde6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 534/534 [05:32<00:00,  1.61it/s, loss=1.1, accuracy=37.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.10, Accuracy: 37.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 534/534 [05:31<00:00,  1.61it/s, loss=1.08, accuracy=44.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.08, Accuracy: 44.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 534/534 [05:32<00:00,  1.61it/s, loss=0.978, accuracy=51.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.98, Accuracy: 51.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 534/534 [05:32<00:00,  1.61it/s, loss=0.772, accuracy=64.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.77, Accuracy: 64.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 534/534 [05:33<00:00,  1.60it/s, loss=0.532, accuracy=73.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.53, Accuracy: 73.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 534/534 [05:32<00:00,  1.61it/s, loss=0.32, accuracy=85.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.32, Accuracy: 85.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 534/534 [05:32<00:00,  1.60it/s, loss=0.169, accuracy=93.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.17, Accuracy: 93.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 534/534 [05:32<00:00,  1.61it/s, loss=0.115, accuracy=96.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.11, Accuracy: 96.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 534/534 [05:32<00:00,  1.61it/s, loss=0.0745, accuracy=98.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.07, Accuracy: 98.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 534/534 [05:32<00:00,  1.61it/s, loss=0.081, accuracy=97.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.08, Accuracy: 97.56%\n",
      "Test Accuracy: 45.20%\n"
     ]
    }
   ],
   "source": [
    "# Bert with dropout and CSV format\n",
    "epochs = 10\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Epoch {}\".format(epoch+1))\n",
    "    for i, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate running loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': running_loss/(i+1), 'accuracy': 100. * correct / total})\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.2f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "    # Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f5bc7",
   "metadata": {},
   "source": [
    "### Data experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1914aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\86189\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\86189\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba4db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    获取一个词的同义词\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, num_replacement=1):\n",
    "    \"\"\"\n",
    "    在句子中随机选择 num_replacement 个词，并用它们的同义词替换\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in nltk.corpus.stopwords.words('english')]))\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        \n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= num_replacement: \n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26683248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of augmented articles: 8532\n"
     ]
    }
   ],
   "source": [
    "augmented_data = []\n",
    "train_json_folder = '../final_project/datasets/dataset_fake_news_task4/train_json'\n",
    "\n",
    "# Loop through each JSON file in the directory\n",
    "for filename in os.listdir(train_json_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(train_json_folder, filename)) as f:\n",
    "            file_data = json.load(f)\n",
    "            for article in file_data['articles']:\n",
    "                # Apply synonym replacement to the title and content of each article\n",
    "                title = synonym_replacement(article['title'])\n",
    "                content = synonym_replacement(article['content'])\n",
    "                augmented_data.append({'title': title, 'content': content, 'label': file_data['label']})\n",
    "\n",
    "# Now, augmented_data contains your training data after synonym replacement\n",
    "# You can write it back to JSON files, or directly use it for training your model\n",
    "print(f\"Number of augmented articles: {len(augmented_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695d2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载增强后的数据\n",
    "train_data = augmented_data\n",
    "\n",
    "# 将标题和内容合并为一个字段\n",
    "train_data = [(item['title'] + ' ' + item['content'], item['label']) for item in train_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d184bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Joe Biden’s Lying Anti-Trump Ad is STILL on Twitter: Gets mellow Fake News Rating of “Four Pinocchios”', 'content': 'Presidential candidate Joe Biden has a well-known history of lying and plagiarism . It looks like his campaign is getting in on the act with their latest ad that just got the worst fake news rating possible . The Washington Post gave Biden ’ s new ad “ Four Pinocchios ” for “ manipulating video ” to make it appear as though President Trump called the coronavirus a hoax . The Biden campaign cut out over 120 words in between the word “ coronavirus ” and then “ This is their new hoax. ” ( see transcript below ) In saying “ coronavirus , ” followed immediately by “ This is their new hoax. ” What the president was saying is that the Democratic politicization of the coronavirus is a “ hoax ” and NOT the virus itself . The ad goes on to show images and words that are disconnected and made to make it seem like the president said “ The American Dream ” … ” is dead. ” This is gutter politics and is still on Twitter ! The full transcript including what the Biden campaign left out is below the coronavirus . You know that , right ? Coronavirus . They ’ re politicizing it . We did one of the great jobs , you say , ‘ How ’ s President Trump doing ? ’ , ‘ Oh , nothing , nothing. ’ They have no clue , they don ’ t have any clue . They can ’ t even count their votes in Iowa , they can ’ t even count . No , they can ’ t . They can ’ t count their votes . One of said , ‘ Mr . President , they tried to beat you on Russia , Russia , Russia. ’ That didn ’ t work out too well . They couldn ’ t do it . They tried the impeachment hoax . That was on a perfect conversation . They tried anything , they tried it over and over , they ’ ve been doing it since he got in . It ’ s all turning , they lost . It ’ s all turning , think of it , think of it . But you know we did something that ’ s been pretty amazing . We have 15 mass in this massive country and because of the fact that we went early , we went early , we could have had a lot more than that. ” Biden ’ s problem with the truth has been completely ignored by the media . They blow it off like it doesn ’ t matter . It ’ s just old “ Uncle Joe ” telling stories again . When Biden ’ s campaign is lying about President race , IT MATTERS .', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(augmented_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd9ee0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53176c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data and convert to tensors\n",
    "train_encodings = tokenizer([text for text, label in train_data], truncation=True, padding=True, max_length=512)\n",
    "train_labels = torch.tensor([label for text, label in train_data])\n",
    "test_encodings = tokenizer([text for text, label in test_data], truncation=True, padding=True, max_length=512)\n",
    "test_labels = torch.tensor([label for text, label in test_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e278ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_encodings['input_ids'] = torch.tensor(train_encodings['input_ids'])\n",
    "train_encodings['attention_mask'] = torch.tensor(train_encodings['attention_mask'])\n",
    "test_encodings['input_ids'] = torch.tensor(test_encodings['input_ids'])\n",
    "test_encodings['attention_mask'] = torch.tensor(test_encodings['attention_mask'])\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e0a8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d253469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 534/534 [05:59<00:00,  1.49it/s, loss=0.855, accuracy=62.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.85, Accuracy: 62.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 534/534 [05:57<00:00,  1.49it/s, loss=0.698, accuracy=69]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.70, Accuracy: 69.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 534/534 [05:59<00:00,  1.49it/s, loss=0.458, accuracy=81.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.46, Accuracy: 81.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 534/534 [06:41<00:00,  1.33it/s, loss=0.253, accuracy=91.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.25, Accuracy: 91.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 534/534 [05:55<00:00,  1.50it/s, loss=0.131, accuracy=96.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.13, Accuracy: 96.39%\n",
      "Test Accuracy: 57.73%\n"
     ]
    }
   ],
   "source": [
    "# Bert with dropout and data augment\n",
    "epochs = 5\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Epoch {}\".format(epoch+1))\n",
    "    for i, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate running loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': running_loss/(i+1), 'accuracy': 100. * correct / total})\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.2f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "    # Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "518d63aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 534/534 [06:17<00:00,  1.41it/s, loss=0.859, accuracy=61.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.86, Accuracy: 61.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 534/534 [05:37<00:00,  1.58it/s, loss=0.698, accuracy=69.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.70, Accuracy: 69.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 534/534 [05:37<00:00,  1.58it/s, loss=0.462, accuracy=82.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.46, Accuracy: 82.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 534/534 [05:37<00:00,  1.58it/s, loss=0.216, accuracy=93.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.22, Accuracy: 93.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 534/534 [05:37<00:00,  1.58it/s, loss=0.0971, accuracy=97.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.10, Accuracy: 97.86%\n",
      "Test Accuracy: 63.59%\n"
     ]
    }
   ],
   "source": [
    "# Bert with dropout\n",
    "epochs = 5\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Epoch {}\".format(epoch+1))\n",
    "    for i, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate running loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': running_loss/(i+1), 'accuracy': 100. * correct / total})\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.2f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "    # Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ff0d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 534/534 [07:25<00:00,  1.20it/s, loss=0.827, accuracy=62.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.83, Accuracy: 62.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 534/534 [06:25<00:00,  1.39it/s, loss=0.628, accuracy=73.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.63, Accuracy: 73.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 534/534 [06:25<00:00,  1.39it/s, loss=0.313, accuracy=88.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.31, Accuracy: 88.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 534/534 [07:03<00:00,  1.26it/s, loss=0.0869, accuracy=97.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.09, Accuracy: 97.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 534/534 [07:06<00:00,  1.25it/s, loss=0.0287, accuracy=99.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.03, Accuracy: 99.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 534/534 [07:04<00:00,  1.26it/s, loss=0.014, accuracy=99.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.01, Accuracy: 99.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 534/534 [07:05<00:00,  1.25it/s, loss=0.0105, accuracy=99.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.01, Accuracy: 99.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 534/534 [07:13<00:00,  1.23it/s, loss=0.00522, accuracy=99.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.01, Accuracy: 99.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 534/534 [06:31<00:00,  1.36it/s, loss=0.00264, accuracy=99.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.00, Accuracy: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 534/534 [05:47<00:00,  1.54it/s, loss=0.00237, accuracy=100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.00, Accuracy: 99.96%\n",
      "Test Accuracy: 61.48%\n"
     ]
    }
   ],
   "source": [
    "# First Trained result\n",
    "epochs = 10\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Epoch {}\".format(epoch+1))\n",
    "    for i, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate running loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': running_loss/(i+1), 'accuracy': 100. * correct / total})\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.2f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "    # Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8eb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
